import argparse
import os
import time
from typing import List, Any

from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb


# -------------------------
# Embedding (for query)
# -------------------------

_embedding_model: SentenceTransformer | None = None


def get_embedding_model() -> SentenceTransformer:
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")
    return _embedding_model


def embed_chunk(chunk: str) -> List[float]:
    embedding_model = get_embedding_model()
    embedding = embedding_model.encode(chunk, normalize_embeddings=True)
    return embedding.tolist()


# -------------------------
# Vector store (Chroma)
# -------------------------

def get_chroma_client():
    return chromadb.PersistentClient(path="./chroma_db")


def retrieve(query: str, top_k: int, collection_name: str = "default") -> List[str]:
    client = get_chroma_client()
    try:
        collection = client.get_collection(name=collection_name)
    except Exception as e:
        raise RuntimeError(f"é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_index.py æ„å»ºç´¢å¼•")
    
    query_embedding = embed_chunk(query)
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    return results['documents'][0]


# -------------------------
# Rerank
# -------------------------

_cross_encoder: CrossEncoder | None = None


def get_cross_encoder() -> CrossEncoder:
    global _cross_encoder
    if _cross_encoder is None:
        _cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
    return _cross_encoder


def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:
    if len(retrieved_chunks) <= top_k:
        return retrieved_chunks
    
    cross_encoder = get_cross_encoder()
    pairs = [(query, chunk) for chunk in retrieved_chunks]
    scores = cross_encoder.predict(pairs)

    scored_chunks = list(zip(retrieved_chunks, scores))
    scored_chunks.sort(key=lambda x: x[1], reverse=True)

    return [chunk for chunk, _ in scored_chunks][:top_k]


# -------------------------
# LLM Providers (Gemini & DeepSeek)
# -------------------------

_google_client: Any = None
_deepseek_client: Any = None


def get_google_client():
    global _google_client
    if _google_client is None:
        from google import genai
        load_dotenv()
        _google_client = genai.Client()
    return _google_client


def get_deepseek_client():
    global _deepseek_client
    if _deepseek_client is None:
        load_dotenv()
        from openai import OpenAI
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        base_url = os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        _deepseek_client = OpenAI(api_key=api_key, base_url=base_url)
    return _deepseek_client


def generate_with_gemini(query: str, chunks: List[str]) -> str:
    # å…ˆåŠ è½½ç¯å¢ƒå˜é‡è·å– API Key
    load_dotenv()
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("ç¼ºå°‘ GEMINI_API_KEY ç¯å¢ƒå˜é‡")
    
    # ä¸º Gemini è®¾ç½®ä¸´æ—¶ä»£ç†
    original_env = {}
    proxy_config = {
        "HTTP_PROXY": os.environ.get("GEMINI_HTTP_PROXY"),
        "HTTPS_PROXY": os.environ.get("GEMINI_HTTPS_PROXY"),
        "GRPC_PROXY": os.environ.get("GEMINI_HTTPS_PROXY"),
    }
    
    # å¤‡ä»½åŸå§‹ç¯å¢ƒå˜é‡å¹¶è®¾ç½®ä»£ç†
    # for key, value in proxy_config.items():
    #     if value:
    #         original_env[key] = os.environ.get(key)
    #         os.environ[key] = value
    #         print(f"ğŸ”§ ä¸º Gemini è®¾ç½®ä»£ç†: {key}={value}")
    
    try:
        # åœ¨ä»£ç†ç¯å¢ƒä¸‹åˆ›å»ºå®¢æˆ·ç«¯
        from google import genai
        google_client = genai.Client(api_key=api_key)  # ä½¿ç”¨å½“å‰ç¯å¢ƒçš„ä»£ç†è®¾ç½®
        joined_chunks = "\n\n".join(chunks)
        prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œä¸‹åˆ—ç‰‡æ®µç”Ÿæˆå‡†ç¡®çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç›¸å…³ç‰‡æ®µ:
{joined_chunks}

è¯·åŸºäºä¸Šè¿°å†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"""

        print(f"\n===== æç¤ºè¯ï¼ˆPromptï¼‰ =====\n{prompt}\n==========================\n")

        response = google_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )

        return response.text
    finally:
        # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
        for key, original_value in original_env.items():
            if original_value is None:
                os.environ.pop(key, None)
            else:
                os.environ[key] = original_value
        
        # æ¸…ç†æ–°è®¾ç½®çš„ä»£ç†ç¯å¢ƒå˜é‡
        for key, value in proxy_config.items():
            if value and key not in original_env:
                os.environ.pop(key, None)


def generate_with_deepseek(query: str, chunks: List[str]) -> str:
    client = get_deepseek_client()
    joined_chunks = "\n\n".join(chunks)

    system_prompt = "ä½ æ˜¯ä¸€ä½çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·ä¾æ®æä¾›çš„ç‰‡æ®µå›ç­”é—®é¢˜ï¼Œé¿å…ç¼–é€ ä¿¡æ¯ã€‚"
    user_prompt = f"ç”¨æˆ·é—®é¢˜: {query}\n\nç›¸å…³ç‰‡æ®µ:\n{joined_chunks}\n\nè¯·åŸºäºä¸Šè¿°å†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"

    print(f"\n===== æç¤ºè¯ï¼ˆPromptï¼‰ =====\n[system]\n{system_prompt}\n\n[user]\n{user_prompt}\n==========================\n")

    completion = client.chat.completions.create(
        model=os.environ.get("DEEPSEEK_MODEL", "deepseek-chat"),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3,
    )
    return completion.choices[0].message.content


def generate(query: str, chunks: List[str], provider: str = "gemini") -> str:
    if provider == "deepseek":
        return generate_with_deepseek(query, chunks)
    return generate_with_gemini(query, chunks)


# -------------------------
# CLI
# -------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="RAG é—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--query", type=str, default="å“†å•¦Aæ¢¦å’Œè¶…çº§èµ›äºšäººçš„å…³ç³»", help="æŸ¥è¯¢é—®é¢˜")
    parser.add_argument("--collection", type=str, default="default", help="é›†åˆåç§°ï¼ˆé»˜è®¤: defaultï¼‰")
    parser.add_argument("--top_k_retrieve", type=int, default=5, help="å¬å›æ¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰")
    parser.add_argument("--top_k_rerank", type=int, default=3, help="é‡æ’åä¿ç•™æ¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰")
    parser.add_argument("--no_generate", action="store_true", help="åªåšæ£€ç´¢ä¸é‡æ’ï¼Œä¸è¿›è¡Œç”Ÿæˆ")
    parser.add_argument("--no_rerank", action="store_true", help="è·³è¿‡é‡æ’æ­¥éª¤")
    parser.add_argument("--provider", type=str, default="gemini", choices=["gemini", "deepseek"], help="ç”Ÿæˆæä¾›æ–¹ï¼ˆgemini/deepseekï¼‰")

    args = parser.parse_args()

    print(f"æŸ¥è¯¢é—®é¢˜: {args.query}")
    print(f"é›†åˆåç§°: {args.collection}")
    print(f"ç”Ÿæˆæä¾›æ–¹: {args.provider}")

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # 1) æ£€ç´¢
    print(f"\næ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ...")
    retrieve_start_time = time.time()
    try:
        retrieved_chunks = retrieve(args.query, args.top_k_retrieve, args.collection)
    except RuntimeError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return
    retrieve_end_time = time.time()
    retrieve_time = retrieve_end_time - retrieve_start_time

    print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieve_time:.3f}ç§’")
    print(f"æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")

    if not retrieved_chunks:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        return

    print("\n===== åˆæ­¥å¬å›ç»“æœ =====")
    for i, chunk in enumerate(retrieved_chunks):
        print(f"[{i}] {chunk}\n")

    # 2) é‡æ’ï¼ˆå¯é€‰ï¼‰
    if args.no_rerank:
        final_chunks = retrieved_chunks[:args.top_k_rerank]
        print("â© è·³è¿‡é‡æ’æ­¥éª¤")
        rerank_time = 0
    else:
        print("æ­£åœ¨é‡æ’...")
        rerank_start_time = time.time()
        final_chunks = rerank(args.query, retrieved_chunks, args.top_k_rerank)
        rerank_end_time = time.time()
        rerank_time = rerank_end_time - rerank_start_time

        print(f"âœ… é‡æ’å®Œæˆï¼Œè€—æ—¶: {rerank_time:.3f}ç§’")
        print("===== é‡æ’åç»“æœ =====")
        for i, chunk in enumerate(final_chunks):
            print(f"[{i}] {chunk}\n")

    # 3) ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
    if args.no_generate:
        print("â© è·³è¿‡ç”Ÿæˆæ­¥éª¤")
        generate_time = 0
    else:
        print(f"æ­£åœ¨ä½¿ç”¨ {args.provider} ç”Ÿæˆå›ç­”...")
        generate_start_time = time.time()
        try:
            answer = generate(args.query, final_chunks, provider=args.provider)
            generate_end_time = time.time()
            generate_time = generate_end_time - generate_start_time
            
            print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generate_time:.3f}ç§’")
            print("===== æœ€ç»ˆå›ç­” =====")
            print(answer)
        except Exception as e:
            generate_end_time = time.time()
            generate_time = generate_end_time - generate_start_time
            print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼ˆè€—æ—¶: {generate_time:.3f}ç§’ï¼‰: {e}")
            return

    # è®¡ç®—å¹¶æ˜¾ç¤ºæ€»è€—æ—¶
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "="*50)
    print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
    print("="*50)
    print(f"ğŸ” æ£€ç´¢è€—æ—¶: {retrieve_time:.3f}ç§’")
    print(f"ğŸ”„ é‡æ’è€—æ—¶: {rerank_time:.3f}ç§’")
    if not args.no_generate:
        print(f"âœ¨ ç”Ÿæˆè€—æ—¶: {generate_time:.3f}ç§’")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}ç§’")
    print("="*50)


if __name__ == "__main__":
    main() 