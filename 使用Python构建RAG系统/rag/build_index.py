import argparse
import json
import os
import time
from typing import List, Any, Dict

from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import chromadb


# -------------------------
# Data preparation
# -------------------------

def split_into_chunks(doc_file: str) -> List[str]:
    with open(doc_file, 'r', encoding='utf-8') as file:
        content = file.read()
    return [chunk for chunk in content.split("\n\n")]


def load_json_data(json_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½ JSON æ–‡ä»¶æ•°æ®"""
    with open(json_file, 'r', encoding='utf-8') as file:
        data = json.load(file)
    return data


def process_json_to_chunks(json_data: List[Dict[str, Any]]) -> tuple[List[str], List[Dict[str, Any]]]:
    """
    å°† JSON æ•°æ®è½¬æ¢ä¸ºæ–‡æ¡£å—å’Œå…ƒæ•°æ®
    è¿”å›: (æ–‡æ¡£å†…å®¹åˆ—è¡¨, å…ƒæ•°æ®åˆ—è¡¨)
    """
    chunks = []
    metadata_list = []
    
    for item in json_data:
        # ç»„åˆæ ‡é¢˜å’Œæè¿°ä½œä¸ºæ–‡æ¡£å†…å®¹
        title = item.get('title', '').strip()
        desc = item.get('desc', '').strip()
        
        # è·³è¿‡ç©ºå†…å®¹
        if not title and not desc:
            continue
            
        # ç»„åˆæ–‡æ¡£å†…å®¹
        content_parts = []
        if title:
            content_parts.append(f"æ ‡é¢˜: {title}")
        if desc:
            content_parts.append(f"å†…å®¹: {desc}")
        
        document_content = "\n".join(content_parts)
        chunks.append(document_content)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'note_id': item.get('note_id', ''),
            'type': item.get('type', ''),
            'title': title,
            'nickname': item.get('nickname', ''),
            'tag_list': item.get('tag_list', ''),
            'source_keyword': item.get('source_keyword', ''),
            'image_list': item.get('image_list', ''),
            'liked_count': item.get('liked_count', '0'),
            'collected_count': item.get('collected_count', '0'),
            'comment_count': item.get('comment_count', '0'),
            'share_count': item.get('share_count', '0'),
            'note_url': item.get('note_url', ''),
            'time': item.get('time', 0)
        }
        metadata_list.append(metadata)
    
    return chunks, metadata_list


# -------------------------
# Embedding
# -------------------------

_embedding_model: SentenceTransformer | None = None


def get_embedding_model() -> SentenceTransformer:
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")
    return _embedding_model


def embed_chunk(chunk: str) -> List[float]:
    embedding_model = get_embedding_model()
    embedding = embedding_model.encode(chunk, normalize_embeddings=True)
    return embedding.tolist()


# -------------------------
# Vector store (Chroma)
# -------------------------

def get_chroma_client():
    return chromadb.PersistentClient(path="./chroma_db")


def save_embeddings(chunks: List[str], embeddings: List[List[float]], collection_name: str = "default", metadata_list: List[Dict[str, Any]] = None) -> None:
    client = get_chroma_client()
    collection = client.get_or_create_collection(name=collection_name)
    
    # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆé‡æ–°æ„å»ºç´¢å¼•ï¼‰
    try:
        client.delete_collection(name=collection_name)
        collection = client.create_collection(name=collection_name)
        print(f"å·²æ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºé›†åˆ: {collection_name}")
    except Exception:
        collection = client.get_or_create_collection(name=collection_name)
        print(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {collection_name}")
    
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}
        
        collection.add(
            documents=[chunk],
            embeddings=[embedding],
            metadatas=[metadata],
            ids=[str(i)]
        )


# -------------------------
# CLI
# -------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="æ„å»ºå‘é‡ç´¢å¼•")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .txt æˆ– .json æ–‡ä»¶ï¼‰")
    parser.add_argument("--collection", type=str, default="default", help="é›†åˆåç§°ï¼ˆé»˜è®¤: defaultï¼‰")
    parser.add_argument("--chunk_method", type=str, default="paragraph", choices=["paragraph", "sentence"], help="åˆ†å—æ–¹æ³•ï¼ˆé»˜è®¤: paragraphï¼Œä»…é€‚ç”¨äºæ–‡æœ¬æ–‡ä»¶ï¼‰")

    args = parser.parse_args()

    if not os.path.exists(args.input):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶: {args.input}")

    print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {args.input}")
    print(f"é›†åˆåç§°: {args.collection}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶å¤„ç†
    file_extension = os.path.splitext(args.input)[1].lower()
    metadata_list = None
    
    if file_extension == '.json':
        print("æ–‡ä»¶ç±»å‹: JSON")
        print("æ­£åœ¨è§£æ JSON æ•°æ®...")
        
        # å¤„ç† JSON æ–‡ä»¶
        json_data = load_json_data(args.input)
        chunks, metadata_list = process_json_to_chunks(json_data)
        print(f"JSON æ¡ç›®æ•°: {len(json_data)}")
        print(f"æœ‰æ•ˆæ–‡æ¡£ç‰‡æ®µæ•°: {len(chunks)}")
        
    else:
        print("æ–‡ä»¶ç±»å‹: æ–‡æœ¬æ–‡ä»¶")
        print(f"åˆ†å—æ–¹æ³•: {args.chunk_method}")
        
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        chunks = split_into_chunks(args.input)
        print(f"å·²åˆ‡åˆ†ç‰‡æ®µæ•°: {len(chunks)}")

    if len(chunks) == 0:
        print("âŒ è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£ç‰‡æ®µ")
        return

    # æ˜¾ç¤ºå‰å‡ ä¸ªç‰‡æ®µçš„é¢„è§ˆ
    print("\n===== æ–‡æ¡£ç‰‡æ®µé¢„è§ˆ =====")
    for i, chunk in enumerate(chunks[:3]):
        print(f"[{i+1}] {chunk[:150]}{'...' if len(chunk) > 150 else ''}\n")

    # 2) å‘é‡åŒ–
    print("ğŸ”„ æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
    embedding_start_time = time.time()
    embeddings = [embed_chunk(chunk) for chunk in chunks]
    embedding_time = time.time() - embedding_start_time
    
    print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {embedding_time:.2f}ç§’")
    print(f"å·²ç”Ÿæˆå‘é‡æ•°: {len(embeddings)}")
    print(f"å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings else 0}")

    # 3) å­˜å…¥å‘é‡åº“
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°å‘é‡æ•°æ®åº“...")
    save_start_time = time.time()
    save_embeddings(chunks, embeddings, args.collection, metadata_list)
    save_time = time.time() - save_start_time
    
    total_time = time.time() - start_time
    
    print(f"âœ… ä¿å­˜å®Œæˆï¼Œè€—æ—¶: {save_time:.2f}ç§’")
    print("\n" + "="*50)
    print("ğŸ“Š æ„å»ºç»Ÿè®¡")
    print("="*50)
    print(f"ğŸ“„ æ–‡æ¡£ç‰‡æ®µ: {len(chunks)} ä¸ª")
    print(f"ğŸ”¢ å‘é‡ç»´åº¦: {len(embeddings[0]) if embeddings else 0}")
    print(f"ğŸ’¾ å­˜å‚¨ä½ç½®: ./chroma_db")
    print(f"ğŸ“‚ é›†åˆåç§°: {args.collection}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    if metadata_list:
        print(f"ğŸ·ï¸  å…ƒæ•°æ®: å·²ä¿å­˜")
    print("="*50)


if __name__ == "__main__":
    main() 