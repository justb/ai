import argparse
import os
import time
from typing import List, Any, Dict, Tuple, Optional

from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, CrossEncoder
import chromadb


# -------------------------
# Embedding (for query)
# -------------------------

_embedding_model: SentenceTransformer | None = None


def get_embedding_model() -> SentenceTransformer:
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")
    return _embedding_model


def embed_chunk(chunk: str) -> List[float]:
    embedding_model = get_embedding_model()
    embedding = embedding_model.encode(chunk, normalize_embeddings=True)
    return embedding.tolist()


# -------------------------
# Vector store (Chroma)
# -------------------------

def get_chroma_client():
    return chromadb.PersistentClient(path="./chroma_db")


def retrieve(query: str, top_k: int, collection_name: str = "default", include_metadata: bool = False) -> Tuple[List[str], Optional[List[Dict[str, Any]]]]:
    """
    æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        collection_name: é›†åˆåç§°
        include_metadata: æ˜¯å¦è¿”å›å…ƒæ•°æ®
    
    Returns:
        å¦‚æœ include_metadata=False: åªè¿”å›æ–‡æ¡£åˆ—è¡¨
        å¦‚æœ include_metadata=True: è¿”å› (æ–‡æ¡£åˆ—è¡¨, å…ƒæ•°æ®åˆ—è¡¨)
    """
    client = get_chroma_client()
    try:
        collection = client.get_collection(name=collection_name)
    except Exception as e:
        raise RuntimeError(f"é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_index.py æ„å»ºç´¢å¼•")
    
    query_embedding = embed_chunk(query)
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    documents = results['documents'][0]
    metadatas = results.get('metadatas', [None] * len(documents))[0] if results.get('metadatas') else None
    
    if include_metadata:
        return documents, metadatas
    else:
        return documents, None


def retrieve_simple(query: str, top_k: int, collection_name: str = "default") -> List[str]:
    """ç®€åŒ–ç‰ˆæ£€ç´¢å‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹æ€§"""
    documents, _ = retrieve(query, top_k, collection_name, include_metadata=False)
    return documents


def extract_images_from_metadata(metadata_list: Optional[List[Dict[str, Any]]]) -> List[List[str]]:
    """ä»å…ƒæ•°æ®ä¸­æå–å›¾ç‰‡URLåˆ—è¡¨"""
    if not metadata_list:
        return []
    
    images_per_chunk = []
    for metadata in metadata_list:
        if metadata and metadata.get('image_list'):
            image_urls = [url.strip() for url in metadata['image_list'].split(',') if url.strip()]
            images_per_chunk.append(image_urls)
        else:
            images_per_chunk.append([])
    
    return images_per_chunk


def post_process_answer_with_images(answer: str, final_chunks: List[str], final_metadata: Optional[List[Dict[str, Any]]]) -> str:
    """åå¤„ç†LLMå›ç­”ï¼Œå¢å¼ºå›¾ç‰‡æ˜¾ç¤º"""
    if not final_metadata:
        return answer
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
    all_images = []
    for i, (chunk, metadata) in enumerate(zip(final_chunks, final_metadata)):
        if metadata and metadata.get('image_list'):
            image_urls = [url.strip() for url in metadata['image_list'].split(',') if url.strip()]
            for j, url in enumerate(image_urls):
                all_images.append({
                    'chunk_index': i + 1,
                    'image_index': j + 1,
                    'url': url,
                    'title': metadata.get('title', f'ç‰‡æ®µ{i+1}'),
                    'author': metadata.get('nickname', 'æœªçŸ¥')
                })
    
    if not all_images:
        return answer
    
    # åœ¨å›ç­”æœ€åæ·»åŠ å›¾ç‰‡å±•ç¤ºåŒºåŸŸ
    enhanced_answer = answer
    
    enhanced_answer += "\n\n" + "="*50
    enhanced_answer += "\nğŸ“¸ ç›¸å…³å›¾ç‰‡"
    enhanced_answer += "\n" + "="*50
    
    current_chunk = None
    for img in all_images:
        if current_chunk != img['chunk_index']:
            current_chunk = img['chunk_index']
            enhanced_answer += f"\n\nğŸ“ æ¥è‡ª: {img['title']} (ä½œè€…: {img['author']})"
        
        enhanced_answer += f"\nğŸ–¼ï¸  å›¾ç‰‡{img['image_index']}: {img['url']}"
    
    enhanced_answer += "\n" + "="*50
    
    return enhanced_answer


def format_chunks_with_images(chunks: List[str], metadata_list: Optional[List[Dict[str, Any]]]) -> str:
    """æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µï¼ŒåŒ…å«å›¾ç‰‡ä¿¡æ¯ç”¨äºLLMå¤„ç†"""
    if not metadata_list:
        return "\n\n".join(chunks)
    
    formatted_chunks = []
    for i, (chunk, metadata) in enumerate(zip(chunks, metadata_list)):
        chunk_text = f"ç‰‡æ®µ{i+1}: {chunk}"
        
        if metadata and metadata.get('image_list'):
            image_urls = [url.strip() for url in metadata['image_list'].split(',') if url.strip()]
            if image_urls:
                chunk_text += f"\n[æ­¤ç‰‡æ®µåŒ…å«{len(image_urls)}å¼ å›¾ç‰‡]"
                for j, url in enumerate(image_urls[:3], 1):  # æœ€å¤šåŒ…å«3å¼ å›¾ç‰‡
                    chunk_text += f"\nå›¾ç‰‡{j}: {url}"
                if len(image_urls) > 3:
                    chunk_text += f"\n...è¿˜æœ‰{len(image_urls)-3}å¼ å›¾ç‰‡"
        
        formatted_chunks.append(chunk_text)
    
    return "\n\n".join(formatted_chunks)


def format_document_with_metadata(doc: str, metadata: Optional[Dict[str, Any]], index: int) -> str:
    """æ ¼å¼åŒ–æ˜¾ç¤ºæ–‡æ¡£å’Œå…ƒæ•°æ®"""
    result = f"[{index}] "
    
    if metadata:
        # æ˜¾ç¤ºæ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        if metadata.get('title'):
            result += f"ğŸ“ {metadata['title']}\n"
        
        # æ˜¾ç¤ºä½œè€…å’Œæ ‡ç­¾ä¿¡æ¯
        info_parts = []
        if metadata.get('nickname'):
            info_parts.append(f"ğŸ‘¤ {metadata['nickname']}")
        if metadata.get('tag_list'):
            info_parts.append(f"ğŸ·ï¸  {metadata['tag_list']}")
        if metadata.get('liked_count'):
            info_parts.append(f"â¤ï¸ {metadata['liked_count']}")
        
        # æ˜¾ç¤ºå›¾ç‰‡æ•°é‡
        if metadata.get('image_list'):
            image_count = len([url.strip() for url in metadata['image_list'].split(',') if url.strip()])
            if image_count > 0:
                info_parts.append(f"ğŸ–¼ï¸ {image_count}å¼ å›¾ç‰‡")
        
        if info_parts:
            result += f"    {' | '.join(info_parts)}\n"
        
        result += f"    {doc}"
    else:
        result += doc
    
    return result


def display_metadata_details(metadata_list: List[Dict[str, Any]], retrieved_chunks: List[str], title: str = "è¯¦ç»†å…ƒæ•°æ®ä¿¡æ¯") -> None:
    """æ˜¾ç¤ºè¯¦ç»†çš„å…ƒæ•°æ®ä¿¡æ¯"""
    if not metadata_list:
        return
    
    print("\n" + "="*60)
    print(f"ğŸ“Š {title}")
    print("="*60)
    
    for i, (metadata, chunk) in enumerate(zip(metadata_list, retrieved_chunks)):
        if not metadata:
            continue
            
        print(f"\nğŸ“„ æ–‡æ¡£ç‰‡æ®µ {i+1}:")
        print("-" * 40)
        
        # åŸºæœ¬ä¿¡æ¯
        if metadata.get('note_id'):
            print(f"ğŸ†” ç¬”è®°ID: {metadata['note_id']}")
        if metadata.get('title'):
            print(f"ğŸ“ æ ‡é¢˜: {metadata['title']}")
        if metadata.get('type'):
            print(f"ğŸ“‚ ç±»å‹: {metadata['type']}")
        if metadata.get('nickname'):
            print(f"ğŸ‘¤ ä½œè€…: {metadata['nickname']}")
        
        # ç¤¾äº¤æ•°æ®
        social_info = []
        if metadata.get('liked_count'):
            social_info.append(f"â¤ï¸ ç‚¹èµ: {metadata['liked_count']}")
        if metadata.get('collected_count'):
            social_info.append(f"ğŸ’¾ æ”¶è—: {metadata['collected_count']}")
        if metadata.get('comment_count'):
            social_info.append(f"ğŸ’¬ è¯„è®º: {metadata['comment_count']}")
        if metadata.get('share_count'):
            social_info.append(f"ğŸ”„ åˆ†äº«: {metadata['share_count']}")
        
        if social_info:
            print(f"ğŸ“Š äº’åŠ¨æ•°æ®: {' | '.join(social_info)}")
        
        # æ ‡ç­¾å’Œå…³é”®è¯
        if metadata.get('tag_list'):
            print(f"ğŸ·ï¸  æ ‡ç­¾: {metadata['tag_list']}")
        if metadata.get('source_keyword'):
            print(f"ğŸ” æ¥æºå…³é”®è¯: {metadata['source_keyword']}")
        
        # æ—¶é—´ä¿¡æ¯
        if metadata.get('time'):
            try:
                import datetime
                timestamp = int(metadata['time']) / 1000  # è½¬æ¢ä¸ºç§’
                date_str = datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                print(f"â° å‘å¸ƒæ—¶é—´: {date_str}")
            except:
                print(f"â° æ—¶é—´æˆ³: {metadata['time']}")
        
        # å›¾ç‰‡ä¿¡æ¯
        if metadata.get('image_list'):
            image_list_str = metadata['image_list']
            # å¤„ç†å¯èƒ½åŒ…å«å¤šä¸ªå›¾ç‰‡URLçš„æƒ…å†µï¼ˆé€—å·åˆ†éš”ï¼‰
            image_urls = [url.strip() for url in image_list_str.split(',') if url.strip()]
            
            if len(image_urls) == 1:
                print(f"ğŸ–¼ï¸  å›¾ç‰‡: {image_urls[0][:60]}{'...' if len(image_urls[0]) > 60 else ''}")
            else:
                print(f"ğŸ–¼ï¸  å›¾ç‰‡ ({len(image_urls)}å¼ ):")
                for idx, url in enumerate(image_urls[:3], 1):  # æœ€å¤šæ˜¾ç¤ºå‰3å¼ 
                    print(f"    [{idx}] {url[:60]}{'...' if len(url) > 60 else ''}")
                if len(image_urls) > 3:
                    print(f"    ... è¿˜æœ‰ {len(image_urls) - 3} å¼ å›¾ç‰‡")
        
        # é“¾æ¥ä¿¡æ¯
        if metadata.get('note_url'):
            print(f"ğŸ”— é“¾æ¥: {metadata['note_url'][:80]}{'...' if len(metadata['note_url']) > 80 else ''}")
        
        print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {chunk[:100]}{'...' if len(chunk) > 100 else ''}")
    
    print("="*60)


# -------------------------
# Rerank
# -------------------------

_cross_encoder: CrossEncoder | None = None


def get_cross_encoder() -> CrossEncoder:
    global _cross_encoder
    if _cross_encoder is None:
        _cross_encoder = CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1')
    return _cross_encoder


def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:
    if len(retrieved_chunks) <= top_k:
        return retrieved_chunks
    
    cross_encoder = get_cross_encoder()
    pairs = [(query, chunk) for chunk in retrieved_chunks]
    scores = cross_encoder.predict(pairs)

    scored_chunks = list(zip(retrieved_chunks, scores))
    scored_chunks.sort(key=lambda x: x[1], reverse=True)

    return [chunk for chunk, _ in scored_chunks][:top_k]


# -------------------------
# LLM Providers (Gemini & DeepSeek)
# -------------------------

_google_client: Any = None
_deepseek_client: Any = None


def get_google_client():
    global _google_client
    if _google_client is None:
        from google import genai
        load_dotenv()
        _google_client = genai.Client()
    return _google_client


def get_deepseek_client():
    global _deepseek_client
    if _deepseek_client is None:
        load_dotenv()
        from openai import OpenAI
        api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not api_key:
            raise RuntimeError("ç¼ºå°‘ DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        base_url = os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        _deepseek_client = OpenAI(api_key=api_key, base_url=base_url)
    return _deepseek_client


def generate_with_gemini(query: str, chunks: List[str], metadata_list: Optional[List[Dict[str, Any]]] = None) -> str:
    # å…ˆåŠ è½½ç¯å¢ƒå˜é‡è·å– API Key
    load_dotenv()
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("ç¼ºå°‘ GEMINI_API_KEY ç¯å¢ƒå˜é‡")
    
    # ä¸º Gemini è®¾ç½®ä¸´æ—¶ä»£ç†
    original_env = {}
    proxy_config = {
        "HTTP_PROXY": os.environ.get("GEMINI_HTTP_PROXY"),
        "HTTPS_PROXY": os.environ.get("GEMINI_HTTPS_PROXY"),
        "GRPC_PROXY": os.environ.get("GEMINI_HTTPS_PROXY"),
    }
    
    # å¤‡ä»½åŸå§‹ç¯å¢ƒå˜é‡å¹¶è®¾ç½®ä»£ç†
    # for key, value in proxy_config.items():
    #     if value:
    #         original_env[key] = os.environ.get(key)
    #         os.environ[key] = value
    #         print(f"ğŸ”§ ä¸º Gemini è®¾ç½®ä»£ç†: {key}={value}")
    
    try:
        # åœ¨ä»£ç†ç¯å¢ƒä¸‹åˆ›å»ºå®¢æˆ·ç«¯
        from google import genai
        google_client = genai.Client(api_key=api_key)  # ä½¿ç”¨å½“å‰ç¯å¢ƒçš„ä»£ç†è®¾ç½®
        
        # æ ¼å¼åŒ–åŒ…å«å›¾ç‰‡ä¿¡æ¯çš„ç‰‡æ®µ
        if metadata_list:
            formatted_chunks = format_chunks_with_images(chunks, metadata_list)
            image_instruction = "\n\næ³¨æ„ï¼šå¦‚æœç›¸å…³ç‰‡æ®µä¸­åŒ…å«å›¾ç‰‡ï¼Œè¯·åœ¨å›ç­”çš„æœ€åæ€»ç»“å„ä¸ªç‰‡æ®µå¹¶åŠ ä¸Šå›¾ç‰‡é“¾æ¥"
        else:
            formatted_chunks = "\n\n".join(chunks)
            image_instruction = ""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œä¸‹åˆ—ç‰‡æ®µç”Ÿæˆå‡†ç¡®çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç›¸å…³ç‰‡æ®µ:
{formatted_chunks}

è¯·åŸºäºä¸Šè¿°å†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚{image_instruction}"""

        print(f"\n===== æç¤ºè¯ï¼ˆPromptï¼‰ =====\n{prompt}\n==========================\n")

        response = google_client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )

        return response.text
    finally:
        # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
        for key, original_value in original_env.items():
            if original_value is None:
                os.environ.pop(key, None)
            else:
                os.environ[key] = original_value
        
        # æ¸…ç†æ–°è®¾ç½®çš„ä»£ç†ç¯å¢ƒå˜é‡
        for key, value in proxy_config.items():
            if value and key not in original_env:
                os.environ.pop(key, None)


def generate_with_deepseek(query: str, chunks: List[str], metadata_list: Optional[List[Dict[str, Any]]] = None) -> str:
    client = get_deepseek_client()
    
    # æ ¼å¼åŒ–åŒ…å«å›¾ç‰‡ä¿¡æ¯çš„ç‰‡æ®µ
    if metadata_list:
        formatted_chunks = format_chunks_with_images(chunks, metadata_list)
        image_instruction = "å¦‚æœç›¸å…³ç‰‡æ®µä¸­åŒ…å«å›¾ç‰‡ï¼Œè¯·åœ¨å›ç­”çš„é€‚å½“ä½ç½®å¼•ç”¨è¿™äº›å›¾ç‰‡ã€‚ä½ å¯ä»¥ä½¿ç”¨[å›¾ç‰‡X]çš„æ ¼å¼æ¥å¼•ç”¨å›¾ç‰‡ï¼Œå¹¶åœ¨å›ç­”æœ€ååˆ—å‡ºæ‰€æœ‰ç›¸å…³å›¾ç‰‡çš„é“¾æ¥ã€‚"
    else:
        formatted_chunks = "\n\n".join(chunks)
        image_instruction = ""

    system_prompt = f"ä½ æ˜¯ä¸€ä½çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·ä¾æ®æä¾›çš„ç‰‡æ®µå›ç­”é—®é¢˜ï¼Œé¿å…ç¼–é€ ä¿¡æ¯ã€‚{image_instruction}"
    user_prompt = f"ç”¨æˆ·é—®é¢˜: {query}\n\nç›¸å…³ç‰‡æ®µ:\n{formatted_chunks}\n\nè¯·åŸºäºä¸Šè¿°å†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚"

    print(f"\n===== æç¤ºè¯ï¼ˆPromptï¼‰ =====\n[system]\n{system_prompt}\n\n[user]\n{user_prompt}\n==========================\n")

    completion = client.chat.completions.create(
        model=os.environ.get("DEEPSEEK_MODEL", "deepseek-chat"),
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3,
    )
    return completion.choices[0].message.content


def generate(query: str, chunks: List[str], provider: str = "gemini", metadata_list: Optional[List[Dict[str, Any]]] = None) -> str:
    if provider == "deepseek":
        return generate_with_deepseek(query, chunks, metadata_list)
    return generate_with_gemini(query, chunks, metadata_list)


# -------------------------
# CLI
# -------------------------

def main() -> None:
    parser = argparse.ArgumentParser(description="RAG é—®ç­”ç³»ç»Ÿ")
    parser.add_argument("--query", type=str, default="å“†å•¦Aæ¢¦å’Œè¶…çº§èµ›äºšäººçš„å…³ç³»", help="æŸ¥è¯¢é—®é¢˜")
    parser.add_argument("--collection", type=str, default="default", help="é›†åˆåç§°ï¼ˆé»˜è®¤: defaultï¼‰")
    parser.add_argument("--top_k_retrieve", type=int, default=2, help="å¬å›æ¡æ•°ï¼ˆé»˜è®¤: 5ï¼‰")
    parser.add_argument("--top_k_rerank", type=int, default=3, help="é‡æ’åä¿ç•™æ¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰")
    parser.add_argument("--no_generate", action="store_true", help="åªåšæ£€ç´¢ä¸é‡æ’ï¼Œä¸è¿›è¡Œç”Ÿæˆ")
    parser.add_argument("--no_rerank", action="store_true", help="è·³è¿‡é‡æ’æ­¥éª¤")
    parser.add_argument("--show_metadata", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„å…ƒæ•°æ®ä¿¡æ¯")
    parser.add_argument("--provider", type=str, default="gemini", choices=["gemini", "deepseek"], help="ç”Ÿæˆæä¾›æ–¹ï¼ˆgemini/deepseekï¼‰")

    args = parser.parse_args()

    print(f"æŸ¥è¯¢é—®é¢˜: {args.query}")
    print(f"é›†åˆåç§°: {args.collection}")
    print(f"ç”Ÿæˆæä¾›æ–¹: {args.provider}")

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # 1) æ£€ç´¢
    print(f"\næ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ...")
    retrieve_start_time = time.time()
    try:
        retrieved_chunks, metadata_list = retrieve(args.query, args.top_k_retrieve, args.collection, include_metadata=True)
    except RuntimeError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return
    retrieve_end_time = time.time()
    retrieve_time = retrieve_end_time - retrieve_start_time

    print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieve_time:.3f}ç§’")
    print(f"æ£€ç´¢åˆ° {len(retrieved_chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")

    if not retrieved_chunks:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        return

    # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®
    has_metadata = metadata_list and any(meta for meta in metadata_list)

    print("\n===== åˆæ­¥å¬å›ç»“æœ =====")
    for i, chunk in enumerate(retrieved_chunks):
        if has_metadata and i < len(metadata_list):
            formatted_result = format_document_with_metadata(chunk, metadata_list[i], i)
            print(f"{formatted_result}\n")
        else:
            print(f"[{i}] {chunk}\n")

    # æ˜¾ç¤ºè¯¦ç»†å…ƒæ•°æ®ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if args.show_metadata and has_metadata:
        display_metadata_details(metadata_list, retrieved_chunks, "åˆæ­¥å¬å›è¯¦ç»†å…ƒæ•°æ®ä¿¡æ¯")

    # 2) é‡æ’ï¼ˆå¯é€‰ï¼‰
    if args.no_rerank:
        final_chunks = retrieved_chunks[:args.top_k_rerank]
        print("â© è·³è¿‡é‡æ’æ­¥éª¤")
        rerank_time = 0
    else:
        print("æ­£åœ¨é‡æ’...")
        rerank_start_time = time.time()
        final_chunks = rerank(args.query, retrieved_chunks, args.top_k_rerank)
        rerank_end_time = time.time()
        rerank_time = rerank_end_time - rerank_start_time

        print(f"âœ… é‡æ’å®Œæˆï¼Œè€—æ—¶: {rerank_time:.3f}ç§’")
        print("===== é‡æ’åç»“æœ =====")
        for i, chunk in enumerate(final_chunks):
            # æ‰¾åˆ°åŸå§‹å…ƒæ•°æ®
            original_index = retrieved_chunks.index(chunk) if chunk in retrieved_chunks else -1
            metadata = None
            if has_metadata and original_index >= 0 and original_index < len(metadata_list):
                metadata = metadata_list[original_index]
            
            if metadata:
                formatted_result = format_document_with_metadata(chunk, metadata, i)
                print(f"{formatted_result}\n")
            else:
                print(f"[{i}] {chunk}\n")

        # æ˜¾ç¤ºé‡æ’åçš„è¯¦ç»†å…ƒæ•°æ®ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
        if args.show_metadata and has_metadata:
            # æ„å»ºé‡æ’åçš„å…ƒæ•°æ®åˆ—è¡¨
            reranked_metadata = []
            for chunk in final_chunks:
                original_index = retrieved_chunks.index(chunk) if chunk in retrieved_chunks else -1
                if original_index >= 0 and original_index < len(metadata_list):
                    reranked_metadata.append(metadata_list[original_index])
                else:
                    reranked_metadata.append({})
            
            display_metadata_details(reranked_metadata, final_chunks, "é‡æ’åè¯¦ç»†å…ƒæ•°æ®ä¿¡æ¯")

    # 3) ç”Ÿæˆï¼ˆå¯é€‰ï¼‰
    if args.no_generate:
        print("â© è·³è¿‡ç”Ÿæˆæ­¥éª¤")
        generate_time = 0
    else:
        print(f"æ­£åœ¨ä½¿ç”¨ {args.provider} ç”Ÿæˆå›ç­”...")
        generate_start_time = time.time()
        try:
            # æ„å»ºæœ€ç»ˆå—å¯¹åº”çš„å…ƒæ•°æ®
            final_metadata = []
            if has_metadata:
                for chunk in final_chunks:
                    original_index = retrieved_chunks.index(chunk) if chunk in retrieved_chunks else -1
                    if original_index >= 0 and original_index < len(metadata_list):
                        final_metadata.append(metadata_list[original_index])
                    else:
                        final_metadata.append({})
            else:
                final_metadata = None
            
            answer = generate(args.query, final_chunks, provider=args.provider, metadata_list=final_metadata)
            
            # åå¤„ç†å›ç­”ï¼Œå¢å¼ºå›¾ç‰‡æ˜¾ç¤º
            # enhanced_answer = post_process_answer_with_images(answer, final_chunks, final_metadata)
            
            generate_end_time = time.time()
            generate_time = generate_end_time - generate_start_time
            
            print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generate_time:.3f}ç§’")
            print("===== æœ€ç»ˆå›ç­” =====")
            print(answer)
        except Exception as e:
            generate_end_time = time.time()
            generate_time = generate_end_time - generate_start_time
            print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼ˆè€—æ—¶: {generate_time:.3f}ç§’ï¼‰: {e}")
            return

    # è®¡ç®—å¹¶æ˜¾ç¤ºæ€»è€—æ—¶
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    print("\n" + "="*50)
    print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡")
    print("="*50)
    print(f"ğŸ” æ£€ç´¢è€—æ—¶: {retrieve_time:.3f}ç§’")
    print(f"ğŸ”„ é‡æ’è€—æ—¶: {rerank_time:.3f}ç§’")
    if not args.no_generate:
        print(f"âœ¨ ç”Ÿæˆè€—æ—¶: {generate_time:.3f}ç§’")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.3f}ç§’")
    print("="*50)


if __name__ == "__main__":
    main() 